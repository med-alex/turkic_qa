{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers evaluate transformers[torch] sacrebleu -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Одиночный перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python /root/turkic_qa/translation_and_train.py --model_name_or_path ychenNLP/nllb-200-3.3B-easyproject --tokenizer_name facebook/nllb-200-distilled-600M --test_file /root/turkic_qa/translation_prep_data/translation_prep_data/xquad_tr/uzn_Latn/contexts/data.json --max_source_length 256 --max_target_length 256 --do_predict --predict_with_generate --generation_max_length 256 --source_lang tur_Latn --target_lang uzn_Latn --use_fast_tokenizer=True --output_dir /root/turkic_qa/translated_data/xquad_tr/uzn_Latn/contexts/ --overwrite_output_dir --per_device_eval_batch_size=24'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = 256\n",
    "data='xquad_tr/uzn_Latn/contexts'\n",
    "target_lang_tag = data.split('/')[1]\n",
    "source_lang_tag = 'tur_Latn'\n",
    "bash_command = 'python /root/turkic_qa/translation_and_train.py' \\\n",
    "              ' --model_name_or_path ychenNLP/nllb-200-3.3B-easyproject' \\\n",
    "              ' --tokenizer_name facebook/nllb-200-distilled-600M' \\\n",
    "              f' --test_file /root/turkic_qa/translation_prep_data/translation_prep_data/{data}/data.json' \\\n",
    "              f' --max_source_length {length}' \\\n",
    "              f' --max_target_length {length}' \\\n",
    "              ' --do_predict' \\\n",
    "              ' --predict_with_generate' \\\n",
    "              f' --generation_max_length {length}' \\\n",
    "              f' --source_lang {source_lang_tag}' \\\n",
    "              f' --target_lang {target_lang_tag}' \\\n",
    "              ' --use_fast_tokenizer=True' \\\n",
    "              f' --output_dir /root/turkic_qa/translated_data/{data}/' \\\n",
    "              ' --overwrite_output_dir' \\\n",
    "              ' --per_device_eval_batch_size=24'\n",
    "              # ' --auto_find_batch_size'\n",
    "\n",
    "bash_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-09 10:22:42.616127: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-09 10:22:42.690600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-09 10:22:43.680497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/09/2024 10:22:45 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': '/root/turkic_qa/translation_prep_data/translation_prep_data/xquad_tr/uzn_Latn/contexts/data.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1190 examples [00:00, 77475.77 examples/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 11759.73it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.22s/it]\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1190/1190 [00:00<00:00, 3506.42 examples/s]\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|██████████| 267/267 [14:47<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =        0.0\n",
      "  predict_gen_len            =    46.9221\n",
      "  predict_loss               =    10.6821\n",
      "  predict_runtime            = 0:14:53.23\n",
      "  predict_samples            =       6408\n",
      "  predict_samples_per_second =      7.174\n",
      "  predict_steps_per_second   =      0.299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(bash_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Потоковой перевод"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-26 08:40:15.967212: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-26 08:40:16.776733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 08:40:18.870330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/26/2024 08:40:23 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': '/root/turkic_qa/translation_prep_data/xquad_en/uzn_Latn/answers/data.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1190 examples [00:00, 43389.10 examples/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 1772.49it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [01:28<00:00, 29.56s/it]\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1190/1190 [00:00<00:00, 21881.73 examples/s]\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|██████████| 25/25 [01:40<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =        0.0\n",
      "  predict_gen_len            =    21.8229\n",
      "  predict_loss               =     7.4075\n",
      "  predict_runtime            = 0:01:47.92\n",
      "  predict_samples            =       1214\n",
      "  predict_samples_per_second =     11.249\n",
      "  predict_steps_per_second   =      0.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-26 08:44:00.495931: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-26 08:44:00.571881: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 08:44:01.575136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/26/2024 08:44:03 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': '/root/turkic_qa/translation_prep_data/xquad_en/uzn_Latn/questions/data.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1190 examples [00:00, 197406.33 examples/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 10727.12it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:15<00:00,  5.13s/it]\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1190/1190 [00:00<00:00, 20351.32 examples/s]\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|██████████| 24/24 [03:34<00:00,  8.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** predict metrics *****\n",
      "  predict_bleu               =        0.0\n",
      "  predict_gen_len            =   251.5254\n",
      "  predict_loss               =    10.2203\n",
      "  predict_runtime            = 0:03:48.63\n",
      "  predict_samples            =       1199\n",
      "  predict_samples_per_second =      5.244\n",
      "  predict_steps_per_second   =      0.105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2024-04-26 08:48:23.179025: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-26 08:48:23.251560: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 08:48:24.248772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/26/2024 08:48:25 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': '/root/turkic_qa/translation_prep_data/xquad_en/uzn_Latn/contexts/data.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 1190 examples [00:00, 135157.24 examples/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [00:00<00:00, 11715.93it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:14<00:00,  4.97s/it]\n",
      "Running tokenizer on prediction dataset: 100%|██████████| 1190/1190 [00:00<00:00, 3647.05 examples/s]\n",
      "/opt/conda/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      " 22%|██▏       | 27/124 [04:21<16:14, 10.05s/it]"
     ]
    }
   ],
   "source": [
    "model_name = 'ychenNLP/nllb-200-3.3B-easyproject'\n",
    "tokenizer_name = 'facebook/nllb-200-distilled-600M'\n",
    "\n",
    "script_dir = '/root/turkic_qa/translation_and_train.py'\n",
    "data_dir = Path('/root/turkic_qa/translation_prep_data/')\n",
    "output_dir = '/root/turkic_qa/translated_data'\n",
    "\n",
    "batch_size = 50\n",
    "num_beams = 5\n",
    "length_penalty = 0.5\n",
    "\n",
    "translated_parts = ['contexts', 'questions', 'answers']\n",
    "translated_datasets = ['xquad_en', 'mlqa_val_en', 'mlqa_test_en_9000', 'squad_val_en_3500', 'squad_train_en_7500']\n",
    "target_langs = ['kaz_Cyrl', 'uzn_Latn']\n",
    "source_lang_tags = ['eng_Latn']\n",
    "for dataset in translated_datasets:\n",
    "    for source_lang_tag in source_lang_tags:\n",
    "        dataset_path = data_dir / dataset\n",
    "        for dirpath, dirnames, filenames in os.walk(dataset_path):\n",
    "            dir_path = Path(dirpath)\n",
    "            if filenames==['data.json'] and dir_path.name in translated_parts and dir_path.parent.name in target_langs:\n",
    "                data_path = Path(f'{dirpath}/{filenames[0]}')\n",
    "\n",
    "                target_lang_tag = data_path.parents[1].name\n",
    "                output_path = Path(data_path.parents[2].name) / data_path.parents[1].name / data_path.parents[0].name\n",
    "\n",
    "                if data_path.parent.name in ['contexts', 'questions']:\n",
    "                    length = 256                    \n",
    "                    bash_command = f'python {script_dir}' \\\n",
    "                                    f' --model_name_or_path {model_name}'\\\n",
    "                                    f' --tokenizer_name {tokenizer_name}' \\\n",
    "                                    f' --test_file {str(data_path)}' \\\n",
    "                                    f' --max_source_length {length}' \\\n",
    "                                    f' --max_target_length {length}' \\\n",
    "                                    ' --do_predict' \\\n",
    "                                    ' --predict_with_generate' \\\n",
    "                                    f' --generation_max_length {length}' \\\n",
    "                                    f' --source_lang {source_lang_tag}' \\\n",
    "                                    f' --target_lang {target_lang_tag}' \\\n",
    "                                    ' --use_fast_tokenizer=True' \\\n",
    "                                    f' --per_device_eval_batch_size={str(batch_size)}' \\\n",
    "                                    f' --output_dir {output_dir}/{str(output_path)}/' \\\n",
    "                                    ' --overwrite_output_dir'\n",
    "                                    \n",
    "                else:\n",
    "                    length = 32\n",
    "                    bash_command = f'python {script_dir}' \\\n",
    "                                    f' --model_name_or_path {model_name}'\\\n",
    "                                    f' --tokenizer_name {tokenizer_name}' \\\n",
    "                                    f' --test_file {str(data_path)}' \\\n",
    "                                    f' --max_source_length {length}' \\\n",
    "                                    f' --max_target_length {length}' \\\n",
    "                                    ' --do_predict' \\\n",
    "                                    ' --predict_with_generate' \\\n",
    "                                    f' --generation_max_length {length}' \\\n",
    "                                    f' --source_lang {source_lang_tag}' \\\n",
    "                                    f' --target_lang {target_lang_tag}' \\\n",
    "                                    ' --use_fast_tokenizer=True' \\\n",
    "                                    f' --per_device_eval_batch_size={str(batch_size)}' \\\n",
    "                                    f' --output_dir {output_dir}/{str(output_path)}/' \\\n",
    "                                    ' --overwrite_output_dir' \\\n",
    "                                    f' --num_beams {num_beams}' \\\n",
    "                                    f' --length_penalty {length_penalty}'\n",
    "\n",
    "                # print(bash_command)\n",
    "                os.system(bash_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
